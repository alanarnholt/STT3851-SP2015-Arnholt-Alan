\documentclass[final, serif, mathserif]{beamer}\usepackage[]{graphicx}\usepackage[]{color}
%% maxwidth is the original width if it is less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

\definecolor{fgcolor}{rgb}{0.345, 0.345, 0.345}
\newcommand{\hlnum}[1]{\textcolor[rgb]{0.686,0.059,0.569}{#1}}%
\newcommand{\hlstr}[1]{\textcolor[rgb]{0.192,0.494,0.8}{#1}}%
\newcommand{\hlcom}[1]{\textcolor[rgb]{0.678,0.584,0.686}{\textit{#1}}}%
\newcommand{\hlopt}[1]{\textcolor[rgb]{0,0,0}{#1}}%
\newcommand{\hlstd}[1]{\textcolor[rgb]{0.345,0.345,0.345}{#1}}%
\newcommand{\hlkwa}[1]{\textcolor[rgb]{0.161,0.373,0.58}{\textbf{#1}}}%
\newcommand{\hlkwb}[1]{\textcolor[rgb]{0.69,0.353,0.396}{#1}}%
\newcommand{\hlkwc}[1]{\textcolor[rgb]{0.333,0.667,0.333}{#1}}%
\newcommand{\hlkwd}[1]{\textcolor[rgb]{0.737,0.353,0.396}{\textbf{#1}}}%

\usepackage{framed}
\makeatletter
\newenvironment{kframe}{%
 \def\at@end@of@kframe{}%
 \ifinner\ifhmode%
  \def\at@end@of@kframe{\end{minipage}}%
  \begin{minipage}{\columnwidth}%
 \fi\fi%
 \def\FrameCommand##1{\hskip\@totalleftmargin \hskip-\fboxsep
 \colorbox{shadecolor}{##1}\hskip-\fboxsep
     % There is no \\@totalrightmargin, so:
     \hskip-\linewidth \hskip-\@totalleftmargin \hskip\columnwidth}%
 \MakeFramed {\advance\hsize-\width
   \@totalleftmargin\z@ \linewidth\hsize
   \@setminipage}}%
 {\par\unskip\endMakeFramed%
 \at@end@of@kframe}
\makeatother

\definecolor{shadecolor}{rgb}{.97, .97, .97}
\definecolor{messagecolor}{rgb}{0, 0, 0}
\definecolor{warningcolor}{rgb}{1, 0, 1}
\definecolor{errorcolor}{rgb}{1, 0, 0}
\newenvironment{knitrout}{}{} % an empty environment to be redefined in TeX

\usepackage{alltt}
%\usefonttheme[onlymath]{serif}
\mode<presentation>{\usetheme{ASU}}
\usepackage{amsmath, amsfonts, amssymb, pxfonts, eulervm, xspace, enumerate, hyperref, color, bookmark}
\usepackage{graphicx}
\usepackage[orientation=landscape, size=a0, scale=1.4, debug]{beamerposter}

\usecolortheme{rose}
\setbeamercolor{background canvas}{bg=magenta!16!yellow!90}

\beamertemplategridbackground[1cm]

%-- Header and footer information ----------------------------------
\newcommand{\footright}{\href{https://github.com/alanarnholt/STT3851-SP2015-Arnholt-Alan}{https://github.com/alanarnholt/STT3851-SP2015-Arnholt-Alan}}
\newcommand{\footleft}{\href{mailto:arnholtat@appstate.edu}{Faculty Advisor: Alan Arnholt}}

\def\conference{16\textsuperscript{th} Annual Celebration of Student Research and Creative Endeavors}
\title{Using Statistical Learning to Predict North Carolina County Voting Patterns}
\author{Will Callaway, Eitan Lees, Maureen O'Donnell} 
\institute{Department of Mathematical Sciences}
%-------------------------------------------------------------------
%\beamertemplategridbackground[1cm]

%-- Main Document --------------------------------------------------
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\begin{document}
\begin{frame}[fragile]
\vspace{-2ex}
\begin{columns}[t]



%-- Column 1 ---------------------------------------------------
\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 1-1
\vspace{0ex}
\begin{block}{Overview}
\begin{itemize}
\item On May 8, 2012, North Carolina voters approved Amendment One.  This poster examines four different models used to predict North Carolina county voting behavior.  
\item To ensure accurate predictive power for future observations, the data are split into a training set (80\%) and a test set (20\%). 
\item Root mean squared error of the test set is used as a measure of model adequacy.  
\item All computations and graphs are created with the open source software \texttt{R} \cite{RC12}. 
\end{itemize}
\vspace{0ex}
\end{block}
%\vspace{3ex}
\vfill

%-- Block 1-2
\begin{block}{K-Fold Cross-Validation}
\begin{itemize}
\item Cross validation is the simplest and most widely used method for estimating prediction error \cite{JF11}.  This method directly estimates the expected extra-sample error, $Err = E[{L(Y, \hat{f\,}\!(X))}]$.  In this work, the loss function, $L$, is the square root of the average squared error loss.
\vspace{2ex}
\item The data in this project is split into $K=10$ equal sized parts.  The cross-validation estimate of the prediction error is $$CV(\hat{f\,}\!)=\frac{1}{N}\sum_{i=1}^{N}L(y_i, \hat{f\,}\!^{-K(i)}(x_i)),$$
where $\hat{f\,}\!^{-K}(x)$ denotes the fitted function with the $K$\textsuperscript{th} part of the data removed.
\end{itemize}
\vspace{0ex}
\vfill
\end{block}
%\vspace{3ex}
\vfill

%-- Block 1-3
\begin{block}{Basic Models Used}
\begin{enumerate}[I.]
\item Least Squares Regression
\vspace{2ex}

Note: Variables are described in the Variable Table handout.
\begin{enumerate}[a.]
\item Model from \cite{DE12} applied to Training data (\textcolor{blue}{mod1A}) --- Percent voting for Amendment One is modeled using the predictors pct18.24, medinc, pctb, mccain08, evanrate, pctrural, and pctba. 
\item Our OLS model applied to Training data (\textcolor{blue}{mod1B}) --- Percent voting for Amendment One is modeled using the predictors obama08, pctrural, pctw, pctd, pctb, log(pct18.24), log(pctcolenrol), pctfm, log(pctfd), pctown,  medinc, $\text{medinc}^2$, evanrate, $\text{evanrate}^2$, pctfb, $\text{pctfb}^2$, log(pctstud), and log(colden).
\end{enumerate}
\item Cross validated, $K = 10$, and pruned, $n_{\text{leaves}}=4$, regression tree \cite{JF11} (\textcolor{blue}{mod2})
\item Random Forest built from, $n_{\text{trees}} = 5000 $, \cite{AL02} (\textcolor{blue}{mod3})
\end{enumerate}
\vspace{0ex}

\end{block}

\end{minipage}
\end{column}%1

%-- Column 2 ---------------------------------------------------

\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 2-1
%\vspace{0ex}
\begin{block}{Cross Validated and Pruned Tree}
%\vspace{0ex}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/TREE-1} 

}



\end{knitrout}
%\vspace{-3ex}
\end{block}

\vfill

%-- Block 2-2
%\vspace{3ex}
\begin{block}{North Carolina Maps}
\vspace{-2ex}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/NCmap-1} 

}




{\centering \includegraphics[width=\maxwidth]{figure/NCmap-2} 

}




{\centering \includegraphics[width=\maxwidth]{figure/NCmap-3} 

}



\end{knitrout}
\vspace{-2ex}
\end{block}
%\vspace{3ex}
\end{minipage}

\end{column}%2

%-- Column 3 ---------------------------------------------------
\begin{column}{0.31\linewidth}
\begin{minipage}[t][.955\textheight]{\linewidth} 
%-- Block 3-1
\vspace{0ex}
\begin{block}{Random Forest Variable Importance}
\vspace{-5ex}
\begin{knitrout}
\definecolor{shadecolor}{rgb}{0.969, 0.969, 0.969}\color{fgcolor}

{\centering \includegraphics[width=\maxwidth]{figure/Rforest-1} 

}



\end{knitrout}
%\vspace{0ex}
\end{block}
%\vspace{3ex}

\vfill

%-- Block 3-2
%\vspace{0ex}
\begin{block}{Prediction Errors}
% latex table generated in R 3.1.2 by xtable 1.7-4 package
% Sun Nov 30 11:49:45 2014
\begin{table}[ht]
\centering
\begin{tabular}{ccccc}
  \hline
 & mod1A & mod1B & mod2 & mod3 \\ 
  \hline
Training Error & 3.56 & 3.05 & 5.51 & 5.48 \\ 
  Testing Error & 4.95 & 3.39 & 7.42 & 5.71 \\ 
   \hline
\end{tabular}
\caption{Training and Testing Error are the RMSE computed from a $K=10$ cross-validated model for all models except mod3.  The RMSE for the random forest model (mod3) does not use cross-validation.} 
\label{PT}
\end{table}

%\vspace{0ex}
\end{block}
%\vspace{3ex}

\vfill
%-- Block 3-3
\begin{block}{Further Directions}
\begin{itemize}
\item Use the models developed in this poster to predict county votes for states that have had similar marriage amendments such as South Carolina, Wisconsin, South Dakota, Florida, Idaho, Alabama, Utah, Michigan, Texas, Arkansas, Louisiana, Kansas, Kentucky, Ohio, and Nebraska. 
\item  Use ensemble methods (combining multiple models) for better prediction.
\item  Make our local maps available via the internet using the shiny server.
\end{itemize}
%\vspace{0ex}
\end{block}
%\vspace{3ex}

\vfill
%-- Block 3-4
\begin{block}{References}
\footnotesize
\begin{thebibliography}{9}
\setbeamertemplate{bibliography item}[text]
\bibitem{RC12}
    \texttt{R} Core Team,
    \emph{\texttt{R}: A Language and Environment for Statistical Computing},
    \texttt{R} Foundation for Statistical Computing,
    Vienna, Austria, 2012,
    \href{http://www.R-project.org}{http://www.R-project.org}.  
\bibitem{JF11}
    Trevor Hastie, Robert Tibshirani, and Jerome Friedman,
    \emph{The Elements of Statistical Learning: Data Mining, Inference , and Prediction},
    New York, New York, 2009.    
\bibitem{DE12}
Davison, E.L. and Jessica N. Eatman \emph{An Ecological Examination of North Carolina's Amendment One Vote to Ban Same Sex Marriage}, (In progress), 2013.
\bibitem{AL02}
    Andy Liaw and Matthew Wiener,
    \emph{Classification and Regression by \texttt{randomForest}}, R News, \textbf{2}, 3, 2002, 18-22.    
\end{thebibliography}
\normalsize
\vspace{0ex}
\end{block}    

\end{minipage}
\end{column}%3




\end{columns}
\end{frame}
\end{document}
\end
